{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Mount Drive and Define Paths\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory for your project\n",
        "base_dir = '/content/drive/MyDrive/DL_project/project'\n",
        "\n",
        "# Define paths to data, images, and metadata files\n",
        "data_dir = os.path.join(base_dir, 'data')\n",
        "images_dir = os.path.join(data_dir, 'images')\n",
        "classes_file = os.path.join(data_dir, 'classes.txt')\n",
        "labels_file = os.path.join(data_dir, 'labels_fixed.csv')\n",
        "attributes_file = os.path.join(data_dir, 'attributes.yaml')\n",
        "\n",
        "# Verify that the critical paths exist before proceeding\n",
        "assert os.path.exists(data_dir), f\"Data directory not found: {data_dir}\"\n",
        "assert os.path.exists(images_dir), f\"Images directory not found: {images_dir}\"\n",
        "assert os.path.exists(classes_file), f\"Classes file not found: {classes_file}\"\n",
        "assert os.path.exists(labels_file), f\"Labels CSV file not found: {labels_file}\"\n",
        "\n",
        "print(\"All paths verified successfully.\")\n"
      ],
      "metadata": {
        "id": "fDwFnSwfwyKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Load Metadata\n",
        "\n",
        "# Read the class names from classes.txt\n",
        "with open(classes_file, 'r') as f:\n",
        "    class_names = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Load the labels and file information from the CSV\n",
        "labels_df = pd.read_csv(labels_file)\n",
        "\n",
        "# Create a mapping from class name to integer index\n",
        "class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "print(f\"Successfully loaded {len(class_names)} classes.\")\n",
        "print(\"Labels DataFrame head:\")\n",
        "print(labels_df.head())\n"
      ],
      "metadata": {
        "id": "NJBdub3Gw3cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Block 3 & 4: Redefine Class and Re-create Datasets ---\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from transformers import DeiTImageProcessor\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, base_dir, class_map, transform=None):\n",
        "        self.df = df\n",
        "        self.base_dir = base_dir\n",
        "        self.class_map = class_map\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        relative_path = row['image_path'].replace('\\\\', '/')\n",
        "        img_path = os.path.join(self.base_dir, relative_path)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.class_map[row['class_label']]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "processor = DeiTImageProcessor.from_pretrained('facebook/deit-tiny-patch16-224')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((processor.size['height'], processor.size['width'])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "])\n",
        "\n",
        "data_folder_path = os.path.join(base_dir, 'data')\n",
        "full_dataset = CustomImageDataset(df=labels_df, base_dir=data_folder_path, class_map=class_to_idx, transform=transform)\n",
        "\n",
        "# Split data\n",
        "train_size = 800\n",
        "val_size = 200\n",
        "train_dataset, val_dataset, _ = random_split(full_dataset, [train_size, val_size, len(full_dataset) - (train_size + val_size)], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"SUCCESS: Datasets and DataLoaders are correctly defined and ready.\")\n"
      ],
      "metadata": {
        "id": "HT06HA-Yd4uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Model Setup with Tuned Hyperparameters\n",
        "import torch.nn as nn\n",
        "from transformers import ViTForImageClassification\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the model, ensuring 'class_names' from your DL_project setup is available\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'facebook/deit-tiny-patch16-224',\n",
        "    num_labels=len(class_names),\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# --- Hyperparameter Adjustments for Overfitting ---\n",
        "# 1. Lower learning rate for more stable convergence\n",
        "# 2. Increased weight decay for stronger regularization\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.1)\n",
        "\n",
        "# 3. Learning rate scheduler to decrease LR over time\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Model re-initialized for DL_project with tuned hyperparameters.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8c0a61ZQyFae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6 : Training and Evaluation with Loss, Accuracy, and F1 Score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "\n",
        "# Ensure you have scikit-learn installed\n",
        "# !pip install -q scikit-learn\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for imgs, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    # Calculate both accuracy and F1 score\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.logits.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    # Calculate both accuracy and F1 score\n",
        "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n"
      ],
      "metadata": {
        "id": "ikpwER1iNjQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7 : Training Loop with Scheduler\n",
        "\n",
        "\n",
        "num_epochs = 12 # Increased epochs to allow for slower learning\n",
        "print(\"Starting fine-tuning process with adjusted hyperparameters...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc, train_f1 = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # --- IMPORTANT: Step the scheduler after the epoch ---\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print all metrics\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "        f\"LR: {optimizer.param_groups[0]['lr']:.1e} | \" # Display current learning rate\n",
        "        f\"Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
        "        f\"Val -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"\\nFine-tuning complete.\")\n"
      ],
      "metadata": {
        "id": "ZmPqnYw6NmpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'best_model_original.pth')\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_original.pth'))"
      ],
      "metadata": {
        "id": "gMy_diFfPhS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Retrieval Block 1: Generate and Save Embeddings ---\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# 1. Initialize Models and Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "print(\"Loading CLIP model and processor for embedding generation...\")\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "print(\"CLIP model loaded.\")\n",
        "\n",
        "# 2. Function to Pre-compute and Save Embeddings\n",
        "def compute_and_save_embeddings(image_dataset, save_path):\n",
        "    \"\"\"\n",
        "    Generates CLIP embeddings for all images in a dataset and saves them to a file.\n",
        "    \"\"\"\n",
        "    clip_model.eval()\n",
        "    all_embeddings = []\n",
        "    all_image_paths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(image_dataset)), desc=\"Generating Embeddings\"):\n",
        "            subset_idx = image_dataset.indices[i]\n",
        "            row = image_dataset.dataset.df.iloc[subset_idx]\n",
        "            base_dir = image_dataset.dataset.base_dir\n",
        "\n",
        "            relative_path = row['image_path'].replace('\\\\', '/')\n",
        "            img_path = os.path.join(base_dir, relative_path)\n",
        "\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            image_inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            image_features = clip_model.get_image_features(**image_inputs)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            all_embeddings.append(image_features.cpu().numpy())\n",
        "            all_image_paths.append(img_path)\n",
        "\n",
        "    embeddings_array = np.vstack(all_embeddings)\n",
        "    np.savez_compressed(save_path, embeddings=embeddings_array, image_paths=all_image_paths)\n",
        "    print(f\"Embeddings for {len(all_image_paths)} images saved to {save_path}\")\n",
        "\n",
        "\n",
        "embeddings_file_path = os.path.join(base_dir, 'validation_embeddings.npz')\n",
        "\n",
        "# Uncomment the line below and run this cell to create the file.\n",
        "compute_and_save_embeddings(val_dataset, embeddings_file_path)\n",
        "\n",
        "print(\"\\nBlock 1 is ready. Uncomment the final line to generate embeddings.\")\n"
      ],
      "metadata": {
        "id": "S60YSZZggCLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Retrieval Block 2: Search Using Saved Embeddings ---\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# 1. Initialize Models and Device (needed for encoding the text query)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "print(\"Loading CLIP model and processor for retrieval...\")\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "print(\"CLIP model loaded.\")\n",
        "\n",
        "# 2. Function to Retrieve Top 3 Images\n",
        "def find_top_3_fast(text_query, embeddings_path):\n",
        "    \"\"\"\n",
        "    Performs a fast search using pre-computed embeddings to find the top 3 matches.\n",
        "    \"\"\"\n",
        "    saved_data = np.load(embeddings_path)\n",
        "    image_embeddings = torch.from_numpy(saved_data['embeddings']).to(device)\n",
        "    image_paths = saved_data['image_paths']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_inputs = clip_processor(text=[text_query], return_tensors=\"pt\").to(device)\n",
        "        text_features = clip_model.get_text_features(**text_inputs)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarities = (text_features @ image_embeddings.T).squeeze(0)\n",
        "        top_scores, top_indices = torch.topk(similarities, k=3)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(top_scores.cpu().tolist(), top_indices.cpu().tolist()):\n",
        "        image = Image.open(image_paths[idx]).convert(\"RGB\")\n",
        "        results.append((score, image))\n",
        "\n",
        "    return results\n",
        "\n",
        "# 3. Example Execution\n",
        "# Assumes 'base_dir' is defined from your training blocks.\n",
        "embeddings_file_path = os.path.join(base_dir, 'validation_embeddings.npz')\n",
        "\n",
        "try:\n",
        "    query = \"a used blue plastic water bottle\"\n",
        "    top_matches = find_top_3_fast(query, embeddings_file_path)\n",
        "\n",
        "    print(f\"\\nTop 3 results for the query: '{query}'\")\n",
        "    if top_matches:\n",
        "        for rank, (score, img) in enumerate(top_matches, 1):\n",
        "            print(f\"\\n--- Rank {rank} | Similarity Score: {score:.4f} ---\")\n",
        "            display(img)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Embeddings file not found at '{embeddings_file_path}'.\")\n",
        "    print(\"Please run Retrieval Block 1 first to generate the embeddings file.\")\n"
      ],
      "metadata": {
        "id": "FbjxsFZIgD7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Analysis Block 1: Get Predictions and Features ---\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_features = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(val_loader, desc=\"Getting Predictions and Features\"):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        # output_hidden_states=True is needed to get the feature embeddings\n",
        "        outputs = model(imgs, output_hidden_states=True)\n",
        "\n",
        "        # Get the feature embeddings from the last hidden layer ([CLS] token)\n",
        "        # This is the representation the classifier uses\n",
        "        features = outputs.hidden_states[-1][:, 0].cpu().numpy()\n",
        "        all_features.append(features)\n",
        "\n",
        "        # Get predictions\n",
        "        preds = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for analysis\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "all_features = np.vstack(all_features)\n",
        "\n",
        "print(\"\\nSUCCESS: Predictions and features have been extracted from the validation set.\")\n",
        "print(f\"Features shape: {all_features.shape}\")\n"
      ],
      "metadata": {
        "id": "G9A0QMh1i_GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Analysis Block 2: Classification Report and Confusion Matrix ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 1. Print Per-Class and Overall Scores\n",
        "print(\"--- Classification Report ---\")\n",
        "# This provides accuracy, precision, recall, and F1-score for each class\n",
        "report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
        "print(report)\n",
        "\n",
        "# 2. Plot the Confusion Matrix\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Fine-Tuned DeiT Model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YkB-yc0XjAGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Analysis Block 5: t-SNE and PCA Feature Visualization ---\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Function to Plot t-SNE ---\n",
        "def plot_tsne(features, labels, class_names):\n",
        "    \"\"\"\n",
        "    Computes and plots a t-SNE visualization of high-dimensional feature data.\n",
        "    \"\"\"\n",
        "    print(\"Running t-SNE... (This may take a moment)\")\n",
        "    # Initialize t-SNE. Perplexity is a key parameter;\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "    features_2d = tsne.fit_transform(features)\n",
        "\n",
        "    # Plot the 2D features\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=features_2d[:, 0],\n",
        "        y=features_2d[:, 1],\n",
        "        hue=[class_names[i] for i in labels],\n",
        "        palette=sns.color_palette(\"hsv\", len(class_names)),\n",
        "        s=50,\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.title('t-SNE Visualization of DeiT Feature Space')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- 2. Function to Plot PCA ---\n",
        "def plot_pca(features, labels, class_names):\n",
        "    \"\"\"\n",
        "    Computes and plots a PCA visualization of high-dimensional feature data.\n",
        "    \"\"\"\n",
        "    print(\"\\nRunning PCA...\")\n",
        "    pca = PCA(n_components=2)\n",
        "    features_2d = pca.fit_transform(features)\n",
        "\n",
        "    # Plot the 2D features\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=features_2d[:, 0],\n",
        "        y=features_2d[:, 1],\n",
        "        hue=[class_names[i] for i in labels],\n",
        "        palette=sns.color_palette(\"hsv\", len(class_names)),\n",
        "        s=50,\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.title('PCA Visualization of DeiT Feature Space')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.legend(title='Classes', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- 3. Example Usage ---\n",
        "\n",
        "\n",
        "# Generate the t-SNE plot\n",
        "plot_tsne(all_features, all_labels, class_names)\n",
        "\n",
        "# Generate the PCA plot\n",
        "plot_pca(all_features, all_labels, class_names)\n"
      ],
      "metadata": {
        "id": "LaPtpqnnjLu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
